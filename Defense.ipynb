{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Of42Ktq5Qr6"
      },
      "outputs": [],
      "source": [
        "!pip install kmedoids\n",
        "!pip install gdown\n",
        "!pip install python-mnist\n",
        "!pip install pulp\n",
        "!pip install scikit-learn==0.22.2 --upgrade\n",
        "!pip install zoopt\n",
        "!pip install pyckmeans\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g71HCBjkaJ88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwHtZKwz5q20"
      },
      "outputs": [],
      "source": [
        "%cd Fair-Clustering-Codebase/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mts-RZ9kTT6w"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL6aOpmUSkq9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8' #:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfGHMVvV51A9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import pandas as pd\n",
        "import random\n",
        "import kmedoids\n",
        "from sklearn.decomposition import PCA\n",
        "from zoopt import Dimension, ValueType, Objective, Parameter, Opt, ExpOpt\n",
        "import seaborn as sns\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from fair_clustering.eval.functions import * #[TO-DO] Write base class and derive metrics from it, temporary eval code\n",
        "\n",
        "from fair_clustering.dataset import ExtendedYaleB, Office31, MNISTUSPS\n",
        "from fair_clustering.algorithm import FairSpectral, FairKCenter, FairletDecomposition, ScalableFairletDecomposition\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMbLij2c7-Wl"
      },
      "outputs": [],
      "source": [
        "# Set parameters related to dataset and get dataset\n",
        "\n",
        "name = 'DIGITS' #Choose between Office-31, MNIST_USPS, Yale, or DIGITS\n",
        "\n",
        "if name == 'Office-31':\n",
        "  dataset = Office31(exclude_domain='amazon', use_feature=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'MNIST_USPS':\n",
        "  dataset = MNISTUSPS(download=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'Yale':\n",
        "  dataset = ExtendedYaleB(download=True, resize=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'DIGITS':\n",
        "  X, y, s = np.load('X_' + name + '.npy'), np.load('y_' + name + '.npy'), np.load('s_' + name + '.npy')\n",
        "\n",
        "print(X.shape, y.shape, s.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aXpU_Zq7sFm"
      },
      "outputs": [],
      "source": [
        "# Fairness Defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di4DWx-1Vfap"
      },
      "outputs": [],
      "source": [
        "from pyckmeans import CKmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVaDiOlF1Zq9"
      },
      "outputs": [],
      "source": [
        "# Remember we need the model class in the same directory to load our models so copy those over\n",
        "!cp Consensus-Fair-Clustering/models.py ./\n",
        "!cp Consensus-Fair-Clustering/utils.py ./\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3keMG_CDacQ"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import GMLP, ClusteringLayer\n",
        "from utils import get_A_r, sparse_mx_to_torch_sparse_tensor, target_distribution, aff\n",
        "\n",
        "from scipy import sparse\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgo6_Z4OLmeT"
      },
      "outputs": [],
      "source": [
        "def Ncontrast(x_dis, adj_label, tau = 1):\n",
        "    \"\"\"\n",
        "    compute the Ncontrast loss\n",
        "    \"\"\"\n",
        "    x_dis = torch.exp( tau * x_dis)\n",
        "    x_dis_sum = torch.sum(x_dis, 1)\n",
        "    x_dis_sum_pos = torch.sum(x_dis*adj_label, 1)\n",
        "    loss = -torch.log(x_dis_sum_pos * (x_dis_sum**(-1))+1e-8).mean()\n",
        "    return loss\n",
        "\n",
        "def get_batch(batch_size, idx_train, adj_label, features):\n",
        "    \"\"\"\n",
        "    get a batch of feature & adjacency matrix\n",
        "    \"\"\"\n",
        "    rand_indx = torch.tensor(np.random.choice(np.arange(adj_label.shape[0]), batch_size)).type(torch.long).cuda()\n",
        "    rand_indx[0:len(idx_train)] = idx_train\n",
        "    features_batch = features[rand_indx]\n",
        "    adj_label_batch = adj_label[rand_indx,:][:,rand_indx]\n",
        "    return features_batch, adj_label_batch\n",
        "\n",
        "def train(model, CL, optimizer, s_idx0, s_idx1, bs, KL_div, tau, alpha, beta, idx_train, adj_label, features, Y, MSEL):\n",
        "    features_batch, adj_label_batch = get_batch(bs, idx_train, adj_label, features)\n",
        "    model.train()\n",
        "    CL.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output, x_dis, embeddings = model(features_batch)\n",
        "    \n",
        "    output = CL(embeddings)\n",
        "    output0, output1 = output[s_idx0], output[s_idx1]\n",
        "    target0, target1 = target_distribution(output0).detach(), target_distribution(output1).detach()\n",
        "    fair_loss = 0.5 * KL_div(output0.log(), target0) + 0.5 * KL_div(output1.log(), target1)\n",
        "\n",
        "    loss_Ncontrast = Ncontrast(x_dis, adj_label_batch, tau = tau)\n",
        "\n",
        "    predict0, predict1 = Y[s_idx0], Y[s_idx1]\n",
        "    partition_loss = 0.5 * MSEL(aff(output0), aff(predict0)) + 0.5 * MSEL(aff(output1), aff(predict1))\n",
        "\n",
        "    loss_train = alpha * fair_loss + loss_Ncontrast + beta * partition_loss\n",
        "\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLy3PyilApIW"
      },
      "outputs": [],
      "source": [
        "def ConsensusFairClusteringHelper(name, X_in, s_in, y_in, save, order=1, lr=0.01, weight_decay=5e-3, alpha=50.0, num_hidden=256, bs=3800, tau=2, epochs=3000, dropout=0.6):\n",
        "  k = len(np.unique(y_in))\n",
        "\n",
        "  if name == 'Office-31':\n",
        "    beta = 100.0 \n",
        "    alpha = 1.0 \n",
        "    order = 1\n",
        "  if name == 'MNIST_USPS':\n",
        "    beta = 25.0 \n",
        "    alpha = 100.0 \n",
        "    order = 2\n",
        "  if name == 'Yale':\n",
        "    beta = 10.0 \n",
        "    alpha = 50.0 \n",
        "    order = 2\n",
        "  if name == 'DIGITS':\n",
        "    beta = 50.0 \n",
        "    alpha = 10.0 \n",
        "    order = 2\n",
        "    num_hidden=36\n",
        "\n",
        "\n",
        "  ckm = CKmeans(k=k, n_rep=100, p_samp=0.5, p_feat=0.5, random_state=42)\n",
        "  ckm.fit(X_in)\n",
        "  ckm_res = ckm.predict(X_in, return_cls=True)\n",
        "\n",
        "\n",
        "  adj, features, labels = ckm_res.cmatrix, X_in, y_in\n",
        "  adj = sparse.csr_matrix(adj)\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
        "  features = torch.FloatTensor(features).float()\n",
        "  labels = torch.LongTensor(labels)\n",
        "  idx_train = np.array(range(len(features)))\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "\n",
        "  adj_label = get_A_r(adj, order)\n",
        "  adj, adj_label, features, idx_train = adj.cuda(), adj_label.cuda(), features.cuda(), idx_train.cuda()\n",
        "\n",
        "  s_idx0, s_idx1 = [], []\n",
        "  for i in range(len(s_in)):\n",
        "    if s_in[i] == 0:\n",
        "      s_idx0.append(i)\n",
        "    elif s_in[i] == 1:\n",
        "      s_idx1.append(i) \n",
        "\n",
        "\n",
        "  L = np.load('Consensus-Fair-Clustering/precomputed_labels/labels_' + name + '.npy')\n",
        "  Y = np.zeros((len(s), k))\n",
        "  for i,l in enumerate(L):\n",
        "    Y[i,l] = 1.0\n",
        "  Y = torch.FloatTensor(Y).float().cuda()\n",
        "  MSEL = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  model = GMLP(nfeat=features.shape[1],\n",
        "              nhid=num_hidden,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=dropout,\n",
        "              )\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  CL = ClusteringLayer(cluster_number=k, hidden_dimension=num_hidden).cuda()\n",
        "  \n",
        "  optimizer = optim.Adam(model.get_parameters() + CL.get_parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  KL_div = nn.KLDivLoss(reduction=\"sum\")\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train(model, CL, optimizer, s_idx0, s_idx1, bs, KL_div, tau, alpha, beta, idx_train, adj_label, features, Y, MSEL)\n",
        "\n",
        "  model.eval()\n",
        "  logits, embeddings = model(features)\n",
        "  CL.eval()\n",
        "  preds = CL(embeddings)\n",
        "  preds = preds.cpu().detach().numpy()\n",
        "  pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "  return pred_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWYGV4EkW8Zt"
      },
      "outputs": [],
      "source": [
        "def ConsensusFairClustering(name, X_in, s_in, y_in, save):\n",
        "  name_bal = {'Office-31': 0.5, 'MNIST_USPS': 0.3, 'DIGITS': 0.1, 'Yale': 0.1}\n",
        "  while True: #Sometimes the model optimizes for a local minima which is why we can run enough times to get a good representation learnt\n",
        "    cfc_labels = ConsensusFairClusteringHelper(name, X_in, s_in, y_in, save)\n",
        "    if balance(cfc_labels, X_in, s_in) >= name_bal[name]: #threshold -> 0.5 for Office-31 and 0.3 (0.4) for MNIST_USPS and 0.1 for DIGITS and 0.1 for Yale\n",
        "      break\n",
        "  print(\"\\nCompleted CFC model training.\")\n",
        "  return cfc_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ2BngWantBP"
      },
      "outputs": [],
      "source": [
        "# Trial run!\n",
        "lbls = ConsensusFairClustering(name, X, s, y, save=False)\n",
        "print(lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykQKdAB2n8Hn"
      },
      "outputs": [],
      "source": [
        "# Check to see metrics too!\n",
        "print(\"balance: {}\".format(balance(lbls, X, s)))\n",
        "print(\"entropy: {}\".format(entropy(lbls, s)))\n",
        "print(\"nmi: {}\".format(nmi(y, lbls)))\n",
        "print(\"acc: {}\".format(acc(y, lbls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ0t_hnMNWDA"
      },
      "outputs": [],
      "source": [
        "def attack_balance(solution):\n",
        "  X_copy, s_copy = X.copy(), s.copy()\n",
        "  flipped_labels = solution.get_x()\n",
        "  i = 0\n",
        "  for idx in U_idx:\n",
        "    s_copy[idx] = flipped_labels[i]\n",
        "    i += 1\n",
        "\n",
        "  labels_sfd = ConsensusFairClustering(name, X_copy, s_copy, y, save=False)\n",
        "  \n",
        "  s_eval = []\n",
        "  X_eval = []\n",
        "  labels_sfd_eval = []\n",
        "  for idx in V_idx:\n",
        "    s_eval.append(s_copy[idx])\n",
        "    X_eval.append(X_copy[idx])\n",
        "    labels_sfd_eval.append(labels_sfd[idx])\n",
        "  s_eval = np.array(s_eval)\n",
        "  X_eval = np.array(X_eval)\n",
        "  labels_sfd_eval = np.array(labels_sfd_eval)\n",
        "\n",
        "  bal = balance(labels_sfd_eval, X_eval, s_eval)\n",
        "\n",
        "  return bal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D3QlGghN_2o"
      },
      "outputs": [],
      "source": [
        "def process_solution(sol):\n",
        "  X_copy, s_copy, y_copy = X.copy(), s.copy(), y.copy()\n",
        "  flipped_labels = sol.get_x()\n",
        "  i = 0\n",
        "  for idx in U_idx:\n",
        "    s_copy[idx] = flipped_labels[i]\n",
        "    i += 1\n",
        "\n",
        "  labels_sfd = ConsensusFairClustering(name, X_copy, s_copy, y, save=False)\n",
        "\n",
        "  s_eval = []\n",
        "  X_eval = []\n",
        "  labels_sfd_eval = []\n",
        "  y_eval = []\n",
        "  for idx in V_idx:\n",
        "    s_eval.append(s_copy[idx])\n",
        "    X_eval.append(X_copy[idx])\n",
        "    labels_sfd_eval.append(labels_sfd[idx])\n",
        "    y_eval.append(y_copy[idx])\n",
        "  s_eval = np.array(s_eval)\n",
        "  X_eval = np.array(X_eval)\n",
        "  labels_sfd_eval = np.array(labels_sfd_eval)\n",
        "  y_eval = np.array(y_eval)\n",
        "\n",
        "  bal = balance(labels_sfd_eval, X_eval, s_eval)\n",
        "  ent = entropy(labels_sfd_eval, s_eval)\n",
        "  accuracy = acc(y_eval, labels_sfd_eval)\n",
        "  nmi_score = nmi(y_eval, labels_sfd_eval)\n",
        "\n",
        "  return (bal, ent, accuracy, nmi_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htHIs8odOGPE"
      },
      "outputs": [],
      "source": [
        "n_clusters = len(np.unique(y))\n",
        "print(\"# of clusters -> \" + str(n_clusters))\n",
        "n_trials = 1\n",
        "\n",
        "U_idx_full, V_idx_full = np.load('U_idx_' + name + '.npy').tolist(), np.load('V_idx_' + name + '.npy').tolist()\n",
        "\n",
        "cfc_pre_res = {\n",
        "    0 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    1 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    2 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    3 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    4 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    5 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    6 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    7 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "}\n",
        "\n",
        "\n",
        "cfc_post_res = {\n",
        "    0 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    1 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    2 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    3 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    4 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    5 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    6 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    7 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "}\n",
        "\n",
        "\n",
        "for percent, j in enumerate([int(0.125*len(U_idx_full)), int(0.25*len(U_idx_full)), int(0.375*len(U_idx_full)), int(0.5*len(U_idx_full)), int(0.625*len(U_idx_full)), int(0.75*len(U_idx_full)), int(0.875*len(U_idx_full)), int(len(U_idx_full))]):\n",
        "  \n",
        "  U_idx = U_idx_full[:j]\n",
        "  V_idx = V_idx_full\n",
        "\n",
        "  for trial_idx in range(n_trials):\n",
        "\n",
        "    labels = ConsensusFairClustering(name, X, s, y, save=False)\n",
        "\n",
        "    s_test = []\n",
        "    X_test = []\n",
        "    labels_test = []\n",
        "    y_test = []\n",
        "    for idx in V_idx:\n",
        "      s_test.append(s[idx])\n",
        "      X_test.append(X[idx])\n",
        "      labels_test.append(labels[idx])\n",
        "      y_test.append(y[idx])\n",
        "    s_test = np.array(s_test)\n",
        "    X_test = np.array(X_test)\n",
        "    labels_test = np.array(labels_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    cfc_pre_res[percent]['BALANCE'].append(balance(labels_test, X_test, s_test))\n",
        "    cfc_pre_res[percent]['ENTROPY'].append(entropy(labels_test, s_test))\n",
        "    cfc_pre_res[percent]['ACC'].append(acc(y_test, labels_test))\n",
        "    cfc_pre_res[percent]['NMI'].append(nmi(y_test, labels_test))\n",
        "\n",
        "    dim_size = len(U_idx)\n",
        "    dim = Dimension(dim_size, [[0, 1]]*dim_size, [False]*dim_size)\n",
        "    obj = Objective(attack_balance, dim)\n",
        "    solution = Opt.min(obj, Parameter(budget=5)) \n",
        "\n",
        "    pa_bal, pa_ent, pa_acc, pa_nmi = process_solution(solution)\n",
        "\n",
        "    cfc_post_res[percent]['BALANCE'].append(pa_bal)\n",
        "    cfc_post_res[percent]['ENTROPY'].append(pa_ent)\n",
        "    cfc_post_res[percent]['ACC'].append(pa_acc)\n",
        "    cfc_post_res[percent]['NMI'].append(pa_nmi)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYZNLnq6W1s8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}